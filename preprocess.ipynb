{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk, re, pprint\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from os import listdir\n",
    "from os.path import isfile, isdir, join\n",
    "import numpy\n",
    "import re\n",
    "import sys\n",
    "import getopt\n",
    "import codecs\n",
    "import time\n",
    "import os\n",
    "import csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "chars = ['{','}','#','%','&','\\(','\\)','\\[','\\]','<','>',',', '!', '.', ';', \n",
    "'?', '*', '\\\\', '\\/', '~', '_','|','=','+','^',':','\\\"','\\'','@','-']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stem(word):\n",
    "   regexp = r'^(.*?)(ing|ly|ed|ious|ies|ive|es|s|ment)?$'\n",
    "   stem, suffix = re.findall(regexp, word)[0]\n",
    "   return stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unique(a):\n",
    "   \"\"\" return the list with duplicate elements removed \"\"\"\n",
    "   return list(set(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def intersect(a, b):\n",
    "   \"\"\" return the intersection of two lists \"\"\"\n",
    "   return list(set(a) & set(b))\n",
    "def union(a, b):\n",
    "   \"\"\" return the union of two lists \"\"\"\n",
    "   return list(set(a) | set(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_files(mypath):\n",
    "   return [ f for f in listdir(mypath) if isfile(join(mypath,f)) ]\n",
    "\n",
    "def get_dirs(mypath):\n",
    "   return [ f for f in listdir(mypath) if isdir(join(mypath,f)) ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reading a bag of words file back into python. The number and order\n",
    "# of sentences should be the same as in the *samples_class* file.\n",
    "def read_bagofwords_dat(myfile):\n",
    "  bagofwords = numpy.genfromtxt('myfile.csv',delimiter=',')\n",
    "  return bagofwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize_corpus(path, train=True):\n",
    "\n",
    "  # used to stem words later on/get roots of words\n",
    "  porter = nltk.PorterStemmer() # also lancaster stemmer\n",
    "  # used to get lemmas (complete words themselves) of words\n",
    "  wnl = nltk.WordNetLemmatizer()\n",
    "  # list of stopwords, can print to view\n",
    "  stopWords = stopwords.words(\"english\")\n",
    "  classes = []\n",
    "  samples = []\n",
    "  docs = []\n",
    "  if train == True:\n",
    "    words = {}\n",
    "  f = open(path, 'r')\n",
    "  lines = f.readlines()\n",
    "\n",
    "    for line in lines:\n",
    "    # separating serial, review and label\n",
    "    classes.append(line.rsplit()[-1])\n",
    "    samples.append(line.rsplit()[0])\n",
    "    raw = line.decode('latin1')\n",
    "    raw = ' '.join(raw.rsplit()[1:-1])\n",
    "    # remove noisy characters; tokenize - specified at the start\n",
    "    raw = re.sub('[%s]' % ''.join(chars), ' ', raw)\n",
    "    tokens = word_tokenize(raw)\n",
    "    # make lower case - !! consider all capitals as more positive or negative?\n",
    "    # !! what if we didn't do this?\n",
    "    tokens = [w.lower() for w in tokens]\n",
    "    # removing stopwords\n",
    "    # using python stopwords scripts - !! add manually to stopwords?\n",
    "    # !! what if we didn't do this?\n",
    "    tokens = [w for w in tokens if w not in stopWords]\n",
    "    # first lemmatize then stem, significance?\n",
    "    # !! what if we didn't do this? - lemmatize\n",
    "    tokens = [wnl.lemmatize(t) for t in tokens]\n",
    "    # !! what if we didn't do this? - stem\n",
    "    tokens = [porter.stem(t) for t in tokens] \n",
    "    \n",
    "    # !! add bigram collocations here\n",
    "    \n",
    "    if train == True:\n",
    "        # add to word count frequency\n",
    "     for t in tokens: \n",
    "         try:\n",
    "             words[t] = words[t]+1\n",
    "         except:\n",
    "             words[t] = 1\n",
    "    docs.append(tokens)\n",
    "    \n",
    "    # docs: consist of a long list of tokens as they\n",
    "    # appear in the text ie may be repeated\n",
    "    # words: dictionary - has the frequency of the vocabulary\n",
    "\n",
    "  if train == True:\n",
    "     return(docs, classes, samples, words)\n",
    "  else:\n",
    "     return(docs, classes, samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.metrics import BigramAssocMeasures\n",
    " \n",
    "def bigram_tokenize_corpus(tokens, score_fn=BigramAssocMeasures.chi_sq, n=200):\n",
    "    bigram_finder = BigramCollocationFinder.from_words(tokens)\n",
    "    # !! selecting within the bigrams\n",
    "    bigrams = bigram_finder.nbest(score_fn, n)\n",
    "    tokens = [ngram for ngram in itertools.chain(words, bigrams)]\n",
    "    \n",
    "    # return dict([(ngram, True) for ngram in itertools.chain(words, bigrams)])\n",
    "    return(tokens)\n",
    " \n",
    "# evaluate_classifier(bigram_word_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ?? !! why is the wordcount threshold hardcoded?\n",
    "def wordcount_filter(words, num=5):\n",
    "   keepset = []\n",
    "   for k in words.keys():\n",
    "       if(words[k] > num):\n",
    "           keepset.append(k)\n",
    "   print \"Vocab length:\", len(keepset)\n",
    "   return(sorted(set(keepset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_wordcounts(docs, vocab):\n",
    "   bagofwords = numpy.zeros(shape=(len(docs),len(vocab)), dtype=numpy.uint8)\n",
    "   vocabIndex={}\n",
    "   for i in range(len(vocab)):\n",
    "      vocabIndex[vocab[i]]=i\n",
    "\n",
    "   for i in range(len(docs)):\n",
    "       doc = docs[i]\n",
    "\n",
    "       for t in doc:\n",
    "          index_t=vocabIndex.get(t)\n",
    "          if index_t>=0:\n",
    "             bagofwords[i,index_t]=bagofwords[i,index_t]+1\n",
    "\n",
    "   print \"Finished find_wordcounts for:\", len(docs), \"docs\"\n",
    "   return(bagofwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main(argv):\n",
    "  \n",
    "  start_time = time.time()\n",
    "\n",
    "  path = ''\n",
    "  outputf = 'out'\n",
    "  vocabf = ''\n",
    "\n",
    "  try:\n",
    "   opts, args = getopt.getopt(argv,\"p:o:v:\",[\"path=\",\"ofile=\",\"vocabfile=\"])\n",
    "  except getopt.GetoptError:\n",
    "    print 'Usage: \\n python preprocessSentences.py -p <path> -o <outputfile> -v <vocabulary>'\n",
    "    sys.exit(2)\n",
    "  for opt, arg in opts:\n",
    "    if opt == '-h':\n",
    "      print 'Usage: \\n python preprocessSentences.py -p <path> -o <outputfile> -v <vocabulary>'\n",
    "      sys.exit()\n",
    "    elif opt in (\"-p\", \"--path\"):\n",
    "      path = arg\n",
    "    elif opt in (\"-o\", \"--ofile\"):\n",
    "      outputf = arg\n",
    "    elif opt in (\"-v\", \"--vocabfile\"):\n",
    "      vocabf = arg\n",
    "\n",
    "  traintxt = path+\"/train3000.txt\"\n",
    "  print 'Path:', path\n",
    "  print 'Training data:', traintxt\n",
    "\n",
    "  # Tokenize training data (if training vocab doesn't already exist):\n",
    "  if (not vocabf):\n",
    "    # !! change threshold?\n",
    "    word_count_threshold = 5\n",
    "    # classes = labels, samples = sample number, \n",
    "    (docs, classes, samples, words) = tokenize_corpus(traintxt, train=True)\n",
    "    vocab = wordcount_filter(words, num=word_count_threshold)\n",
    "    # Write new vocab file\n",
    "    vocabf = outputf+\"_vocab_\"+str(word_count_threshold)+\".txt\"\n",
    "    outfile = codecs.open(path+\"/\"+vocabf, 'w',\"utf-8-sig\")\n",
    "    outfile.write(\"\\n\".join(vocab))\n",
    "    outfile.close()\n",
    "  else:\n",
    "    word_count_threshold = 0\n",
    "    (docs, classes, samples) = tokenize_corpus(traintxt, train=False)\n",
    "    vocabfile = open(path+\"/\"+vocabf, 'r')\n",
    "    vocab = [line.rstrip('\\n') for line in vocabfile]\n",
    "    vocabfile.close()\n",
    "\n",
    "  print 'Vocabulary file:', path+\"/\"+vocabf \n",
    "\n",
    "  # Get bag of words:\n",
    "  bow = find_wordcounts(docs, vocab)\n",
    "  # Check: sum over docs to check if any zero word counts\n",
    "  print \"Doc with smallest number of words in vocab has:\", min(numpy.sum(bow, axis=1))\n",
    "\n",
    "  # Write bow file\n",
    "  with open(path+\"/\"+outputf+\"_bag_of_words_\"+str(word_count_threshold)+\".csv\", \"wb\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(bow)\n",
    "\n",
    "  # Write classes\n",
    "  outfile = open(path+\"/\"+outputf+\"_classes_\"+str(word_count_threshold)+\".txt\", 'w')\n",
    "  outfile.write(\"\\n\".join(classes))\n",
    "  outfile.close()\n",
    "\n",
    "  # Write samples\n",
    "  outfile = open(path+\"/\"+outputf+\"_samples_class_\"+str(word_count_threshold)+\".txt\", 'w')\n",
    "  outfile.write(\"\\n\".join(samples))\n",
    "  outfile.close()\n",
    "\n",
    "  print 'Output files:', path+\"/\"+outputf+\"*\"\n",
    "\n",
    "  # Runtime\n",
    "  print 'Runtime:', str(time.time() - start_time)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  main(sys.argv[1:])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
